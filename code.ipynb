{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00f499a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hn/f0pwk4x16qd2mgc2p5mlqlm40000gn/T/ipykernel_3120/781164355.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### Import Libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "### Import Libraries\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0680e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a new SparkSession and assign it to a variable named spark\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cd5785",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create an RDD from a list of sample clickstream counts and save it as clickstream_counts_rdd.\n",
    "\n",
    "# Sample clickstream counts\n",
    "sample_clickstream_counts = [\n",
    "    [\"other-search\", \"Hanging_Gardens_of_Babylon\", \"external\", 47000],\n",
    "    [\"other-empty\", \"Hanging_Gardens_of_Babylon\", \"external\", 34600],\n",
    "    [\"Wonders_of_the_World\", \"Hanging_Gardens_of_Babylon\", \"link\", 14000],\n",
    "    [\"Babylon\", \"Hanging_Gardens_of_Babylon\", \"link\", 2500]\n",
    "]\n",
    "\n",
    "# Create RDD from sample data\n",
    "clickstream_counts_rdd = spark.sparkContext.parallelize(sample_clickstream_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23a4ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using the RDD from the previous step, create a DataFrame named clickstream_sample_df\n",
    "\n",
    "# Create a DataFrame from the RDD of sample clickstream counts\n",
    "clickstream_sample_df = clickstream_counts_rdd.toDF(['source_page', 'target_page', 'link_category','link_count']) \n",
    "\n",
    "# Display the DataFrame to the notebook\n",
    "clickstream_sample_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b33fb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read the files in ./cleaned/clickstream/ into a new Spark DataFrame named clickstream\n",
    "### and display the first few rows of the DataFrame in the notebook\n",
    "\n",
    "# Read the target directory (`./cleaned/clickstream/`) into a DataFrame (`clickstream`)\n",
    "clickstream = spark.read \\\n",
    "    .option('header', True) \\\n",
    "    .option('delimiter', '\\t') \\\n",
    "    .option('inferSchema', True) \\\n",
    "    .csv(\"./cleaned/clickstream/\")\n",
    "\n",
    "# Display the DataFrame to the notebook\n",
    "clickstream.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee7bbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Print the schema of the DataFrame in the notebook.\n",
    "\n",
    "# Display the schema of the `clickstream` DataFrame to the notebook\n",
    "clickstream.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21bed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Drop the language_code column from the DataFrame and display the new schema in the notebook\n",
    "\n",
    "# Drop target columns\n",
    "clickstream = clickstream.drop('language_code')\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "clickstream.show(5)\n",
    "# Display the new schema in the notebook\n",
    "clickstream.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4284432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rename `referrer` and `resource` to `source_page` and `target_page`, respectively,\n",
    "\n",
    "# Rename `referrer` and `resource` to `source_page` and `target_page`\n",
    "clickstream = clickstream\\\n",
    ".withColumnRenamed('referrer', 'source_page')\\\n",
    ".withColumnRenamed('resource','target_page')\n",
    "  \n",
    "# Display the first few rows of the DataFrame\n",
    "clickstream.show(5, truncate=False)\n",
    "# Display the new schema in the notebook\n",
    "clickstream.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd1087c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add the clickstream DataFrame as a temporary view named clickstream\n",
    "### to make the data queryable with sparkSession.sql()\n",
    "\n",
    "# Create a temporary view in the metadata for this `SparkSession` \n",
    "clickstream.createOrReplaceTempView('clickstream')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2320c9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filter the dataset to entries with Hanging_Gardens_of_Babylon\n",
    "### as the target_page and order the result by click_count using\n",
    "### PySpark DataFrame methods.\n",
    "\n",
    "# Filter and sort the DataFrame using PySpark DataFrame methods\n",
    "clickstream\\\n",
    ".filter(clickstream.target_page == 'Hanging_Gardens_of_Babylon')\\\n",
    ".sort('click_count', ascending=False)\\\n",
    ".show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d65ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Perform the same analysis as the previous exercise using a SQL query.\n",
    "\n",
    "# Filter and sort the DataFrame using SQL\n",
    "spark.sql(\n",
    "\"\"\"\n",
    "SELECT * \n",
    "FROM clickstream\n",
    "WHERE target_page == 'Hanging_Gardens_of_Babylon'\n",
    "ORDER BY click_count DESC\n",
    "\"\"\"\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ed08fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate the sum of click_count grouped by link_category using PySpark DataFrame methods.\n",
    "\n",
    "# Aggregate the DataFrame using SQL\n",
    "spark.sql(\n",
    "\"\"\"\n",
    "SELECT link_category, sum(click_count)\n",
    "FROM clickstream\n",
    "GROUP BY link_category\n",
    "\"\"\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d07c6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's create a new DataFrame named internal_clickstream that only contains article pairs where link_category is link.\n",
    "\n",
    "# Create a new DataFrame (named `internal_clickstream`) using `filter` to select rows to \n",
    "# a specific condition and `select` to choose which columns to return from the query.\n",
    "internal_clickstream = clickstream\\\n",
    "    .select([\"source_page\", \"target_page\", \"click_count\"])\\\n",
    "    .filter(clickstream.link_category == 'link')\n",
    "\n",
    "# Display the first few rows of the DataFrame in the notebook\n",
    "internal_clickstream.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca17745",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the internal_clickstream DataFrame as CSV files in a directory called ./results/article_to_article_csv/\n",
    "\n",
    "# Save the `internal_clickstream` DataFrame to a series of CSV files in `./results/article_links_csv/`\n",
    "# with `DataFrame.write.csv()`\n",
    "internal_clickstream\\\n",
    "    .write\\\n",
    "    .csv('./results/article_links_csv/', mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d715bbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the internal_clickstream DataFrame as parquet files in a directory called ./results/article_to_article_pq/\n",
    "\n",
    "# Save the `internal_clickstream` DataFrame to a series of parquet files in `./results/article_links_parquet/`\n",
    "# with `DataFrame.write.parquet()`\n",
    "\n",
    "internal_clickstream\\\n",
    "    .write\\\n",
    "    .parquet('./results/article_links_parquet/', mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51290f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Close the SparkSession and underlying sparkContext\n",
    "\n",
    "# Stop the notebook's `SparkSession` and `SparkContext`\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
